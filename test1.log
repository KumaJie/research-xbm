2025-01-05 11:51:01,720 Train INFO: DATA:
  NUM_INSTANCES: 4
  NUM_WORKERS: 16
  PKUVID_IMG_SOURCE: 
  QUERY_IMG_SOURCE: 
  SAMPLE: RandomIdentitySampler
  TEST_BATCHSIZE: 256
  TEST_IMG_SOURCE: /root/shared-nvme/Stanford_Online_Products/Ebay_test.txt
  TRAIN_BATCHSIZE: 64
  TRAIN_IMG_SOURCE: /root/shared-nvme/Stanford_Online_Products/Ebay_train.txt
INPUT:
  CROP_SCALE: [0.2, 1]
  CROP_SIZE: [224, 224]
  FLIP_PROB: 0.5
  MODE: RGB
  ORIGIN_SIZE: [256, 256]
  PIXEL_MEAN: [0.485, 0.456, 0.406]
  PIXEL_STD: [0.229, 0.224, 0.225]
LOGGER:
  LEVEL: 20
  STREAM: stdout
LOSSES:
  MULTI_SIMILARITY_LOSS:
    HARD_MINING: True
    SCALE_NEG: 40.0
    SCALE_POS: 2.0
  NAME: contrastive_loss
MODEL:
  BACKBONE:
    LAST_STRIDE: 2
    NAME: resnet50
  DEVICE: cuda
  HEAD:
    DIM: 128
    IN_CHANNELS: 2048
    NAME: linear_norm
    NUM_CLASSES: 1000
  PRETRAIN: imagenet
  PRETRIANED_PATH:
    bninception: ~/.torch/models/bn_inception-52deb4733.pth
    googlenet: ~/.torch/models/googlenet-1378be20.pth
    resnet50: ~/.torch/models/pytorch_resnet50.pth
  WEIGHT: 
NAME: save
SAVE: True
SAVE_DIR: /root/shared-nvme/research-xbm
SOLVER:
  BASE_LR: 0.0001
  BIAS_LR_FACTOR: 1
  CHECKPOINT_PERIOD: 1000
  FINETURN_MODE_PATH: 
  FIX_BN: False
  GAMMA: 0.1
  IS_FINETURN: False
  MAX_ITERS: 35000
  MOMENTUM: 0.9
  OPTIMIZER_NAME: Adam
  RNG_SEED: 1
  STEPS: 2
  WEIGHT_DECAY: 0.0005
  WEIGHT_DECAY_BIAS: 0.0005
TB_SAVE_DIR: /root/shared-nvme/research-xbm
VALIDATION:
  IS_VALIDATION: True
  R: [1]
  VERBOSE: 1000
XBM:
  ENABLE: True
  SIZE: 55000
  START_ITERATION: 1000
  WEIGHT: 1.0
2025-01-05 11:51:02,467 Train INFO: | Dataset Info |datasize: 59551|num_labels: 11318|
2025-01-05 11:51:02,468 Train INFO: | Dataset Info |datasize: 60502|num_labels: 11316|
2025-01-05 11:51:02,471 Train INFO: Start training
2025-01-05 11:51:02,471 Train INFO: >>> use XBM
2025-01-05 11:51:08,970 Train INFO: eta: 3:07:15  iter: 20  time: 0.1750 (0.3212)  data: 0.0790 (0.1475)  loss: 2.3759 (5.8676)  lr: 0.000100  max mem: 5.6 GB
2025-01-05 11:51:12,511 Train INFO: eta: 2:25:11  iter: 40  time: 0.1770 (0.2492)  data: 0.0792 (0.1133)  loss: 1.9624 (3.9675)  lr: 0.000100  max mem: 5.6 GB
2025-01-05 11:51:16,062 Train INFO: eta: 2:11:11  iter: 60  time: 0.1774 (0.2253)  data: 0.0788 (0.1019)  loss: 2.1265 (3.3959)  lr: 0.000100  max mem: 5.6 GB
2025-01-05 11:51:19,604 Train INFO: eta: 2:04:06  iter: 80  time: 0.1760 (0.2132)  data: 0.0803 (0.0964)  loss: 2.0424 (3.0904)  lr: 0.000100  max mem: 5.6 GB
2025-01-05 11:51:23,166 Train INFO: eta: 1:59:56  iter: 100  time: 0.1776 (0.2062)  data: 0.0793 (0.0930)  loss: 2.2281 (2.9202)  lr: 0.000100  max mem: 5.6 GB
2025-01-05 11:51:26,729 Train INFO: eta: 1:57:09  iter: 120  time: 0.1775 (0.2015)  data: 0.0797 (0.0908)  loss: 2.1988 (2.8087)  lr: 0.000100  max mem: 5.6 GB
2025-01-05 11:51:30,268 Train INFO: eta: 1:55:02  iter: 140  time: 0.1765 (0.1980)  data: 0.0799 (0.0893)  loss: 2.0373 (2.7198)  lr: 0.000100  max mem: 5.6 GB
2025-01-05 11:51:33,814 Train INFO: eta: 1:53:28  iter: 160  time: 0.1771 (0.1954)  data: 0.0803 (0.0882)  loss: 2.2231 (2.6525)  lr: 0.000100  max mem: 5.6 GB
2025-01-05 11:51:37,387 Train INFO: eta: 1:52:19  iter: 180  time: 0.1774 (0.1936)  data: 0.0802 (0.0873)  loss: 2.0835 (2.5928)  lr: 0.000100  max mem: 5.6 GB
2025-01-05 11:51:40,947 Train INFO: eta: 1:51:21  iter: 200  time: 0.1772 (0.1920)  data: 0.0798 (0.0865)  loss: 2.2166 (2.5488)  lr: 0.000100  max mem: 5.6 GB
2025-01-05 11:51:44,514 Train INFO: eta: 1:50:35  iter: 220  time: 0.1777 (0.1908)  data: 0.0807 (0.0860)  loss: 2.0906 (2.5039)  lr: 0.000100  max mem: 5.6 GB
2025-01-05 11:51:48,083 Train INFO: eta: 1:49:55  iter: 240  time: 0.1781 (0.1897)  data: 0.0796 (0.0855)  loss: 2.0284 (2.4706)  lr: 0.000100  max mem: 5.6 GB
2025-01-05 11:51:51,633 Train INFO: eta: 1:49:18  iter: 260  time: 0.1772 (0.1888)  data: 0.0806 (0.0851)  loss: 2.0782 (2.4368)  lr: 0.000100  max mem: 5.6 GB
2025-01-05 11:51:55,180 Train INFO: eta: 1:48:46  iter: 280  time: 0.1765 (0.1880)  data: 0.0800 (0.0848)  loss: 1.8877 (2.4059)  lr: 0.000100  max mem: 5.6 GB
2025-01-05 11:51:58,728 Train INFO: eta: 1:48:18  iter: 300  time: 0.1773 (0.1873)  data: 0.0800 (0.0844)  loss: 2.1210 (2.3877)  lr: 0.000100  max mem: 5.6 GB
2025-01-05 11:52:02,284 Train INFO: eta: 1:47:54  iter: 320  time: 0.1772 (0.1867)  data: 0.0798 (0.0842)  loss: 2.1088 (2.3661)  lr: 0.000100  max mem: 5.6 GB
2025-01-05 11:52:05,814 Train INFO: eta: 1:47:29  iter: 340  time: 0.1762 (0.1861)  data: 0.0794 (0.0839)  loss: 1.9338 (2.3415)  lr: 0.000100  max mem: 5.6 GB
2025-01-05 11:52:09,344 Train INFO: eta: 1:47:07  iter: 360  time: 0.1769 (0.1856)  data: 0.0797 (0.0837)  loss: 1.9090 (2.3195)  lr: 0.000100  max mem: 5.6 GB
2025-01-05 11:52:12,873 Train INFO: eta: 1:46:47  iter: 380  time: 0.1761 (0.1851)  data: 0.0799 (0.0835)  loss: 2.0040 (2.3030)  lr: 0.000100  max mem: 5.6 GB
2025-01-05 11:52:16,406 Train INFO: eta: 1:46:29  iter: 400  time: 0.1763 (0.1847)  data: 0.0792 (0.0833)  loss: 1.8748 (2.2839)  lr: 0.000100  max mem: 5.6 GB
2025-01-05 11:52:19,922 Train INFO: eta: 1:46:10  iter: 420  time: 0.1751 (0.1842)  data: 0.0796 (0.0831)  loss: 2.0743 (2.2731)  lr: 0.000100  max mem: 5.6 GB
2025-01-05 11:52:23,456 Train INFO: eta: 1:45:55  iter: 440  time: 0.1759 (0.1839)  data: 0.0796 (0.0829)  loss: 2.0380 (2.2621)  lr: 0.000100  max mem: 5.6 GB
2025-01-05 11:52:27,000 Train INFO: eta: 1:45:41  iter: 460  time: 0.1763 (0.1836)  data: 0.0788 (0.0828)  loss: 1.9098 (2.2521)  lr: 0.000100  max mem: 5.6 GB
2025-01-05 11:52:30,509 Train INFO: eta: 1:45:25  iter: 480  time: 0.1750 (0.1833)  data: 0.0790 (0.0826)  loss: 1.8301 (2.2370)  lr: 0.000100  max mem: 5.6 GB
2025-01-05 11:52:34,022 Train INFO: eta: 1:45:11  iter: 500  time: 0.1754 (0.1830)  data: 0.0794 (0.0825)  loss: 1.8957 (2.2242)  lr: 0.000100  max mem: 5.6 GB
2025-01-05 11:52:37,542 Train INFO: eta: 1:44:58  iter: 520  time: 0.1757 (0.1827)  data: 0.0793 (0.0824)  loss: 1.9082 (2.2131)  lr: 0.000100  max mem: 5.6 GB
2025-01-05 11:52:41,074 Train INFO: eta: 1:44:47  iter: 540  time: 0.1757 (0.1825)  data: 0.0791 (0.0823)  loss: 1.8567 (2.2002)  lr: 0.000100  max mem: 5.6 GB
2025-01-05 11:52:44,609 Train INFO: eta: 1:44:36  iter: 560  time: 0.1772 (0.1823)  data: 0.0788 (0.0821)  loss: 1.8497 (2.1923)  lr: 0.000100  max mem: 5.6 GB
2025-01-05 11:52:48,142 Train INFO: eta: 1:44:26  iter: 580  time: 0.1759 (0.1821)  data: 0.0790 (0.0820)  loss: 2.0144 (2.1819)  lr: 0.000100  max mem: 5.6 GB
2025-01-05 11:52:51,661 Train INFO: eta: 1:44:15  iter: 600  time: 0.1756 (0.1819)  data: 0.0790 (0.0819)  loss: 1.9009 (2.1721)  lr: 0.000100  max mem: 5.6 GB
2025-01-05 11:52:55,189 Train INFO: eta: 1:44:06  iter: 620  time: 0.1769 (0.1817)  data: 0.0784 (0.0818)  loss: 1.8604 (2.1624)  lr: 0.000100  max mem: 5.6 GB
2025-01-05 11:52:58,726 Train INFO: eta: 1:43:57  iter: 640  time: 0.1761 (0.1815)  data: 0.0787 (0.0817)  loss: 1.9358 (2.1575)  lr: 0.000100  max mem: 5.6 GB
2025-01-05 11:53:02,243 Train INFO: eta: 1:43:47  iter: 660  time: 0.1758 (0.1814)  data: 0.0785 (0.0816)  loss: 1.9678 (2.1523)  lr: 0.000100  max mem: 5.6 GB
2025-01-05 11:53:05,776 Train INFO: eta: 1:43:39  iter: 680  time: 0.1759 (0.1812)  data: 0.0784 (0.0815)  loss: 1.7507 (2.1429)  lr: 0.000100  max mem: 5.6 GB
2025-01-05 11:53:09,311 Train INFO: eta: 1:43:31  iter: 700  time: 0.1757 (0.1811)  data: 0.0782 (0.0815)  loss: 1.8835 (2.1359)  lr: 0.000100  max mem: 5.6 GB
2025-01-05 11:53:12,835 Train INFO: eta: 1:43:23  iter: 720  time: 0.1756 (0.1810)  data: 0.0784 (0.0814)  loss: 1.8732 (2.1285)  lr: 0.000100  max mem: 5.6 GB
2025-01-05 11:53:16,363 Train INFO: eta: 1:43:15  iter: 740  time: 0.1754 (0.1808)  data: 0.0782 (0.0813)  loss: 1.9722 (2.1229)  lr: 0.000100  max mem: 5.6 GB
2025-01-05 11:53:19,884 Train INFO: eta: 1:43:07  iter: 760  time: 0.1752 (0.1807)  data: 0.0784 (0.0812)  loss: 1.9058 (2.1181)  lr: 0.000100  max mem: 5.6 GB
2025-01-05 11:53:23,391 Train INFO: eta: 1:42:59  iter: 780  time: 0.1748 (0.1806)  data: 0.0781 (0.0811)  loss: 1.9241 (2.1125)  lr: 0.000100  max mem: 5.6 GB
2025-01-05 11:53:26,898 Train INFO: eta: 1:42:51  iter: 800  time: 0.1750 (0.1804)  data: 0.0777 (0.0811)  loss: 1.9267 (2.1068)  lr: 0.000100  max mem: 5.6 GB
2025-01-05 11:53:30,409 Train INFO: eta: 1:42:43  iter: 820  time: 0.1753 (0.1803)  data: 0.0782 (0.0810)  loss: 1.8543 (2.1012)  lr: 0.000100  max mem: 5.6 GB
2025-01-05 11:53:33,929 Train INFO: eta: 1:42:36  iter: 840  time: 0.1748 (0.1802)  data: 0.0775 (0.0809)  loss: 1.6669 (2.0936)  lr: 0.000100  max mem: 5.6 GB
2025-01-05 11:53:37,443 Train INFO: eta: 1:42:29  iter: 860  time: 0.1749 (0.1801)  data: 0.0773 (0.0808)  loss: 1.7871 (2.0862)  lr: 0.000100  max mem: 5.6 GB
2025-01-05 11:53:40,973 Train INFO: eta: 1:42:22  iter: 880  time: 0.1756 (0.1800)  data: 0.0774 (0.0807)  loss: 1.9574 (2.0828)  lr: 0.000100  max mem: 5.6 GB
2025-01-05 11:53:44,486 Train INFO: eta: 1:42:15  iter: 900  time: 0.1754 (0.1799)  data: 0.0772 (0.0807)  loss: 1.9287 (2.0776)  lr: 0.000100  max mem: 5.6 GB
2025-01-05 11:53:48,001 Train INFO: eta: 1:42:09  iter: 920  time: 0.1753 (0.1798)  data: 0.0768 (0.0806)  loss: 1.8280 (2.0723)  lr: 0.000100  max mem: 5.6 GB
2025-01-05 11:53:51,489 Train INFO: eta: 1:42:01  iter: 940  time: 0.1737 (0.1797)  data: 0.0771 (0.0805)  loss: 1.7433 (2.0660)  lr: 0.000100  max mem: 5.6 GB
2025-01-05 11:53:54,988 Train INFO: eta: 1:41:54  iter: 960  time: 0.1744 (0.1796)  data: 0.0771 (0.0804)  loss: 1.8123 (2.0614)  lr: 0.000100  max mem: 5.6 GB
2025-01-05 11:53:58,519 Train INFO: eta: 1:41:48  iter: 980  time: 0.1760 (0.1796)  data: 0.0763 (0.0804)  loss: 1.9301 (2.0599)  lr: 0.000100  max mem: 5.6 GB
2025-01-05 11:54:02,008 Train INFO: eta: 1:41:41  iter: 1000  time: 0.1746 (0.1795)  data: 0.0762 (0.0803)  loss: 1.7539 (2.0544)  lr: 0.000100  max mem: 5.6 GB
2025-01-05 11:54:02,411 Train INFO: Validation
2025-01-05 11:54:02,416 Train INFO: Begin extract
Faiss assertion 'err == CUBLAS_STATUS_SUCCESS' failed in void faiss::gpu::runMatrixMult(faiss::gpu::Tensor<float, 2, true>&, bool, faiss::gpu::Tensor<T, 2, true>&, bool, faiss::gpu::Tensor<IndexType, 2, true>&, bool, float, float, cublasHandle_t, cudaStream_t) [with AT = float; BT = float; cublasHandle_t = cublasContext*; cudaStream_t = CUstream_st*] at /project/faiss/faiss/gpu/utils/MatrixMult-inl.cuh:265; details: cublas failed (13): (512, 256) x (60502, 256)' = (512, 60502) gemm params m 60502 n 512 k 256 trA T trB N lda 256 ldb 256 ldc 60502
